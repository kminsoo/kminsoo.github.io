<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
   <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
   <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
   <link rel="stylesheet" href="./jemdoc.css" type="text/css" />
   <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto">
   <title>Minsoo Kang</title>
</head>

<body>
   <div id="layout-content">
      <p><br />
      </p>
      <script>
         (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
               (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date(); a = s.createElement(o),
               m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
         })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

         ga('create', 'UA-78549539-1', 'auto');
         ga('send', 'pageview');

      </script>
      <table class="imgtable">
         <tr>
            <td>
               <a href="https://kminsoo.github.io"><img src="imgs/minsookang.jpg" alt="" width="150px"
                     height="200px" /></a>&nbsp;
            </td>
            <td align="left">
               <p><b>
                     <font size="6"> Minsoo Kang </font>
                  </b>
                  <br /> <br />
                  Ph.D. Candidate at Seoul National University <br />
                  Email : <a href="mailto:kminsoo@snu.ac.kr">kminsoo [at] snu [dot] ac [dot] kr </a> <br />
               </p>
            </td>
         </tr>
      </table>

      <!--
         <h1>News</h1>
         <ul>
            <li><p>[Feb 2020] One paper is accepted to CVPR 2020.
            <li><p>[Nov 2019] One paper is accepted to AAAI 2020.
            <li><p>[Apr 2019] Our paper entiled "Streamlined Dense Video Captioning" is now available on arXiv. This work is done during my internship program at Sanp Research.
                  <a class="news" target="_blank" href="http://arxiv.org/abs/1904.03870">[arXiv preprint]</a>
            </li>
            <li><p>[Apr 2019] We release the dataset and code for the paper "Transfer Learning via Unsupervised Task Discovery for Visual Question Answering" 
                  and the paper is available on arXiv.
                  <a class="news" target="_blank" href="https://arxiv.org/abs/1810.02358">[arXiv preprint]</a>
                  <a class="news" target="_blank" href="https://github.com/HyeonwooNoh/vqa_task_discovery">[code]</a>
            </li>
            <li><p>[Feb 2019] Two papers "Streamlined Dense Video Captioning" <font color=red>(oral)</font> and "Transfer Learning via Unsupervised Task Discovery for Visual Question Answering" are accepted to CVPR 2019.
            </li>
            <li><p>[Sep 2018] Our paper "Learning to Specialize with Knowledge Distillation for Visual Question Answering" is accepted to NeurIPS 2018.
            </li>
            <li><p>[Jun 2018] I am working at Snap Research in Venice as a research intern.
            </li>
            Our paper <strong>"Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization"</strong> is accepted to <strong>NIPS 2017</strong>.
<a class="news" target="_blank" href="https://arxiv.org/abs/1710.05179">[paper]</a><a class="news" target="_blank" href="./papers/noh2017regularizing/noh2017regularizing_poster.pdf">[poster]</a><a class="news" target="_blank" href="https://youtu.be/QA8K4zQmLc4">[video]</a/>
            <li><p>[Dec 2016] Our paper about new dataset of video question answering, which is called as MarioQA, is now available at <b><a href="http://arxiv.org/abs/1612.01669">arXiv</a></b> </p>
            </li>
            <li><p>[Nov 2016] Our paper about image captioning has been accepted to AAAI 2017 </p>
            </li>
         </ul>
         -->


      <!-- Education Section -->
      <h1>Education</h1>
      <ul>
         <li>
            <p><b>M.S. &amp; Ph.D. integrated course</b>, <a href="http://cv.snu.ac.kr">Computer Vision Lab</a>, <a
															 href="http://snu.ac.kr">Seoul National University</a> (Mar. 2018 &ndash; Feb. 2024 (expected))</p>
         </li>
         <ul>
            <li>Advisor : <a href="http://cv.snu.ac.kr/~bhhan">Prof. Bohyung Han</a>
         </ul>
         <li>
            <p><b>B.S</b>. <a href="http://ee.postech.ac.kr">School of Electrical Engineering</a>, <a
                  href="http://postech.ac.kr">POSTECH</a> (Mar. 2011 &ndash; Feb. 2018)</p>
         </li>
      </ul>

      <!-- Work Experiences Section -->
      <h1>Work Experiences</h1>
      <ul>
         <li>
            <p><b>Research Intern</b>, Kakao Brain, Pangyo, Seongnam, Korea (Jan. 2022 &ndash; Aug. 2022)</p>
         </li>
      </ul>


      <!-- Publication Section -->
      <h1>Publications</h1>


      <!-- CSG-->
      <table class="imgtable">
         <tr>
            <td>
               <img src="imgs/NeurIPS2023.png" alt="" width="280px" height="110px" />&nbsp;
            </td>
            <td align="left">
               <p><b><a href="https://arxiv.org/abs/2305.18007">
                        Conditional Score Guidance for Text-Driven Image-to-Image Translation
                     </a></b> <br />
                  Hyunsoo Lee*, <u><b>Minsoo Kang*</b></u>, Bohyung Han (* equal contribution) <br />
                  In NeurIPS 2023 <br />
                  <a class="news" target="_blank" href="https://arxiv.org/abs/2305.18007">[arXiv preprint]</a>
               </p>
            </td>
         </tr>
      </table>
      <p><br /></p>

      <!-- VDL -->
      <table class="imgtable">
         <tr>
            <td>
               <img src="imgs/CVPR2023.png" alt="" width="280px" height="110px" />&nbsp;
            </td>
            <td align="left">
               <p><b><a href="https://arxiv.org/abs/2303.16105">
						Variational Distribution Learning for Unsupervised Text-to-Image Generation
                     </a></b> <br />
					 <u><b>Minsoo Kang</b></u>, Doyup Lee, Jiseob Kim, Saehoon Kim, Bohyung Han<br />
                  In CVPR 2023 <br />
                  <a class="news" target="_blank" href="https://arxiv.org/abs/2303.16105">[arXiv preprint]</a>
               </p>
            </td>
         </tr>
      </table>
      <p><br /></p>

      <!-- VEM-->
      <table class="imgtable">
         <tr>
            <td>
               <img src="imgs/NeurIPS2022.png" alt="" width="280px" height="110px" />&nbsp;
            </td>
            <td align="left">
               <p><b><a href="https://arxiv.org/abs/2303.16050">
                        Information-Theoretic GAN Compression with Variational Energy-based Model</a></b> <br />
                  <u><b>Minsoo Kang</b></u>, Hyewon Yoo, Eunhee Kang, Sehwan Ki, Hyong-Euk Lee, Bohyung Han<br />
                  In NuerIPS 2022 <br />
                  <a target="_blank"
                     href="https://arxiv.org/abs/2303.16050">[paper]</a>
				  <a class="news" target="_blank" href="https://openreview.net/attachment?id=sRKNkpUMQNr&name=supplementary_material">[code]</a>
               </p>
            </td>
         </tr>
      </table>
      <p><br /></p>

      <!-- AFC-->
      <table class="imgtable">
         <tr>
            <td>
               <img src="imgs/CVPR2022.png" alt="" width="280px" height="110px" />&nbsp;
            </td>
            <td align="left">
               <p><b><a href="https://arxiv.org/abs/2204.00895">
                        Class-Incremental Learning by Knowledge Distillation with Adaptive Feature Consolidation</a></b> <br />
                  <u><b>Minsoo Kang</b></u>, Jaeyoo Park, Bohyung Han<br />
                  In CVPR 2022, <font color=red size=+0.5><b>Oral presentation</b></font> <br />
                  <a class="news" target="_blank" href="https://arxiv.org/abs/2204.00895">[arXiv preprint]</a>
				  <a class="news" target="_blank" href="https://github.com/kminsoo/afc">[code]</a>
               </p>
            </td>
         </tr>
      </table>
      <p><br /></p>


      <!-- VIDEO-->
      <table class="imgtable">
         <tr>
            <td>
               <img src="imgs/ICCV2021.png" alt="" width="280px" height="130px" />&nbsp;
            </td>
            <td align="left">
               <p><b><a href="https://arxiv.org/abs/2203.13611">
                        Class-Incremental Learning for Action Recognition in Videos</a></b> <br />
				  Jaeyoo Park, <u><b>Minsoo Kang</b></u>, Bohyung Han<br />
                  In ICCV 2021 <br />
                  <a target="_blank" href="https://arxiv.org/abs/2203.13611">[arXiv preprint]</a>
				  <a class="news" target="_blank" href="https://github.com/bellos1203/TCD">[code]</a>
               </p>
            </td>
         </tr>
      </table>
      <p><br /></p>


      <!-- SCP -->
      <table class="imgtable">
         <tr>
            <td>
               <img src="imgs/ICML2020.png" alt="" width="280px" height="130px" />&nbsp;
            </td>
            <td align="left">
               <p><b><a href="https://arxiv.org/abs/2007.03938">
					   Operation-Aware Soft Channel Pruning using Differentiable Masks</a></b> <br />
                  <u><b>Minsoo Kang</b></u>, Bohyung Han<br />
                  In ICML2020 <br />
                  <a href="https://arxiv.org/abs/2007.03938">[arXiv preprint]</a>
				  <a class="news" target="_blank" href="https://github.com/kminsoo/scp">[code]</a>
               </p>
            </td>
         </tr>
      </table>
      <p><br /></p>


      <!-- KDAS -->
      <table class="imgtable">
         <tr>
            <td>
               <img src="imgs/AAAI2020.png" alt="" width="280px" height="110px" />&nbsp;
            </td>
            <td align="left">
               <p><b><a href="https://arxiv.org/abs/1911.13019">
                        Towards Oracle Knowledge Distillation with Neural Architecture Search
                     </a></b> <br />
				  <u><b>Minsoo Kang*</b></u>, Jonghwan Mun*, Bohyung Han (* equal contribution) <br />
                  In AAAI 2020 <br />
                  <a class="news" target="_blank" href="https://arxiv.org/abs/1911.13019">[arXiv preprint]</a>
               </p>
            </td>
         </tr>
      </table>
      <p><br /></p>



      <!-- Awards and Honors Section -->
      <h1>Awards and Honors</h1>
      <ul>
         <li>
            <p><b>Google Ph.D. Fellowship Finalist</b>, 2023</p>
         </li>
         <li>
            <p><b>NeurIPS Scholar Award</b>, 2022</p>
         </li>
         <li>
            <p><b>Youlchon Scholarship</b>, 2022</p>
         </li>
         <li>
            <p><b>Google Student Travel Grants for CVPR</b>, 2022</p>
         </li>
         <li>
            <p><b>Jigok Scholarship</b>, POSTECH, 2011 â€“
               2017</p>
         </li>
         <li>
            <p><b>Silver Medal, Korean Mathematical Olympiad (KMO)</b>, 2010</p>
         </li>
      </ul>


      <!-- Projects Section -->
      <!--
         <h1>Projects</h1>
         <ul>
            <li><p><b>Video Question Answering with Temporal Reasoning</b> <br />
            Software R&amp;D Center, Samsung Electronics Co., South Korea <br />
            (Aug 2016 - Dec 2016)</p>
            </li>
         </ul>
         <ul>
            <li><p><b>Development of Image Caption Generation Algorithm</b> <br />
            DMC R&amp;D Center, Samsung Electronics Co., South Korea <br />
            (Aug 2015 - Jul 2016)</p>
            </li>
         </ul>
		 -->
   </div>
</body>

</html>
